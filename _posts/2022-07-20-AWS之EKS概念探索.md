---
title: 003_AWS之EKS概念探索
date: '2022-07-20 16:12:00 +0800'
categories: [Cloud]
tags: [AWS,Kubernetes]     # TAG names should always be lowercase
author: owen
mermaid: true
math: true
---

# 起源
在想要進一步透過 terraform 配置 AWS EKS 叢集的相關設定時，對於在使用場景和部分名詞定義上的觀念有模糊不清且對其具體影響也不了解，故留下此篇文章來加強自己對於 EKS 這個服務的功能更進一步的了解並在往後合適的時機能夠加以應用。

# 關於 node 的種類及其差異
總的來說，node 種類分為三種。分別是 self-managed、managed、fargate。而參考過各種資料後，自己理解是「差別在於對 kubernetes 的 control plane 控制權(又或是控制的程度)」。
AWS 具體的差異比對 <https://docs.aws.amazon.com/zh_tw/eks/latest/userguide/eks-compute.html>
- self-managed : 對於 control plane 有最大的控制，基本上對於 AWS 來說，AWS 認為採用此種類屬於用戶自己管理整個 kubernetes，因為 AWS 並不會干涉用戶創建的任何資源和方式。
- managed : 對於 control plane 有一定程度的干涉，並且因為有干涉所以對於部署方式及配置有一定程度的限制。但也因為如此，所以 AWS 把這種節點的 ec2 instance 幫忙管理，也會在更新時遵循用戶的配置將 instance 和 pod 做控管。
- fargate : 基本上對用戶來說就是看不到 instance，因為這種模式需要直接定義一個稱為 「fargate profile」的設定檔，只要遵循該設定啟動的 pods，都會用 fargate 的形式部署，與前面兩種最大的差異是它是按 vCPU/硬碟 收費的。並且因為用戶只能管 pods，所以滿多限制條件。但好處是可以最大程度的做到資源利用。(因為無 instance 所以又稱為 serverless)

## 小結
基本上 managed 和 self-managed 都是一樣的費用，它是按 instance 資源的收費標準。所以機器規格多大、用多少硬碟等等的就直接是這兩種的費用。

但因為兩者之間對於 control plane 還是有些許差異，所以目前讀起來感受的最大差異應該是在「如果要用 dedicated host(專用主機)」來部署 EKS，則就必須要用 self-managed 的方式。managed 不提供 dedicated host 部署。

另外像是 managed 也不提供 windows container 和 用 local zone 部署，但不知道未來有沒有機會開放。

# 關於 AWS EKS 的 Pods 限制
AWS EKS 在預設情況下，對於 Pods 部署的數量是有限制的，公式為 ENI * (可用私有IP數量 - 1) + 2。

而每個 instance type 能夠用的 ENI 數量和可用私有 IP 數量限制可以參考 <https://docs.aws.amazon.com/zh_tw/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI>

舉例來說一台 t3a.large 最多可以部署的 pods 總數量為 「3 * (12-1) + 2」 = 35

# 關於 EKS 的 Addons (附加元件)
因為 terraform 的範例有段配置 addons 的相關設定，故自己往下瞭解。發現 AWS EKS 在自己初始化的時候就會安裝基本的 addons 來確保 cluster 在建置完成後就可以使用基本功能，而這些 addons 是基於 AWS Cloud 的配置，故部分 addons 功能是必要的。

參考 addons 說明 <https://docs.aws.amazon.com/zh_tw/eks/latest/userguide/eks-add-ons.html>

像是 VPC CNI / CoreDNS / kube-proxy 這三個在用 EKS 搭建的時候就會自己安裝。
- VPC CNI : 讓 kubernetes 內的 pod 可以直接與 VPC 上的 IP 一樣
- CoreDNS : 提供 kubernetes 內的 pod 做 DNS 解析
- kube-proxy : 會維護每個 Amazon EC2 節點上的網路規則。它使網路能與您的 Pod 進行通訊。

> 以上情況是 managed / self-managed 的情況，若用 Fargate 情況會不太一樣
{: .prompt-info }

# 關於 EKS 的 auto scale 設定機制
要做到 EKS 自動擴展，主要有兩種做法。一種是透過安裝 Kubernetes Cluster Autoscaler，而另一種較新，它採用一個開源專案 Karpenter。

參考 <https://docs.aws.amazon.com/zh_tw/eks/latest/userguide/autoscaling.html#ca-deployment-considerations>

## 關於 EKS Cluster OIDC Provider
因為要做 Cluster Autoscaler(CA) 功能之前，前置條件必須要讓 cluster 有 OIDC Provider，如果沒有就無法用 CA。
參考 - 先決條件 <https://docs.aws.amazon.com/zh_tw/eks/latest/userguide/autoscaling.html>

創建方式可以參考
- <https://docs.aws.amazon.com/zh_tw/eks/latest/userguide/enable-iam-roles-for-service-accounts.html>

主要流程為
1. 先到 eks 頁面複製 eks cluster 資訊裡面的 oidc provider url
2. 到 iam 介面的 identity provider 頁面新增一個 provider
3. provider type (供應商類型) : open id connect
4. provider url (供應商 URL) : 剛剛複製的 url
5. audience (對象) : "sts.amazonaws.com"

頁面示意圖
![](/commons/image/20220721/001_eks-oidc-provider.png)

## 設定 Cluster Autoscaler 的原則

### 基本條件
1. 要先有上面提到的 Cluster OIDC Provider - 這邊用 terraform 創建的 eks node group，所以可以直接用參數 enable
![](/commons/image/20220725/000_TF_irsa.png)
2. 要讓創建出來的 node 都有特定標籤，才能夠自動發現這些 node
![](/commons/image/20220722/000_autoscaler_tag.png)
3. 要有 IAM 權限可以更新 autoscaling group 的相對設定值。(這邊有兩種方法)
    - 一種是直接給 autoscaling 創建出來的 instance 賦予該權限 => IAM Role for EC2 instance
    - 另一種是給 cluster-autoscaler 的 pod 的 service account 有 iam role 的權限 => 透過 IRSA IAM Role for Service account

參考 Auto-Discovery Setup 章節 <https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/aws>

> 每個 ASG 應該要由提供相同 capacity 的 instance type 組成，例如 xlarge 系列 `m5a.xlarge/m4.xlarge/m5.xlarge/m5d.xlarge`，因為它們都提供 4vCPUs 和 16GiB 的 RAM
{: .prompt-tip }


### 流程大綱
1. 建立一個 IAM Role，這個 Role 會給 cluster Autoscaler 的 pod 中 service account 針對 aws 的 auto scaling 相對應的權限，讓這個 pod 可以操作 aws 的 auto scaling
2. 部署 Cluster Autoscaler 到 EKS 裡面，這邊會設定對應的 IAM Role arn 給 service account，因為要讓 service account 跟 iam role 做關聯 (這邊就是 IRSA 功能)
3. 查看 Cluster Autoscaler 是否正常 running

> 建立 Policy
![](/commons/image/20220725/000_CA_installation.png)

> 建立 Role 要 attach 剛剛創建的 Policy
![](/commons/image/20220725/001_CA_installation.png)
![](/commons/image/20220725/002_CA_installation.png)

> 修改 Role 的 Trust Relationships
![](/commons/image/20220725/003_CA_installation.png)

> 用 helm charts 部署 cluster-autoscaler 時，要針對 rbac 設定
![](/commons/image/20220725/004_CA_installation.png)

> 針對整個 charts value 作一些參數調整
![](/commons/image/20220725/005_CA_installation.png)
![](/commons/image/20220725/006_CA_installation.png)
![](/commons/image/20220725/007_CA_installation.png)

> 查看 cluster-autoscaler Pod 相關訊息
![](/commons/image/20220725/008_CA_installation.png)
![](/commons/image/20220725/009_CA_installation.png)

> 補充
{: .prompt-tip }
- 關於部署 CA 的考量 (CA 最佳建議)，分為 3 個方向
    - 擴展考量
    - 效能考量
    - 成本效益及可用性


1. 設定 node group 時，在每個 AZ 設定一組，啟用 `--balance-similar-node-groups` 功能。若是只有一個 node group，則要設定跨多個 AZ。
2. 在 scaling group 內的每個 node 都具備相同的配置，包含 label、taints、resources
3. 原則上配置盡量用少量的 node group。然後在 node group 裡面的 node(instance) 數量可以配置多一點。(若設定相反，則可能造成 scale 一些問題，官網沒明說那些問題)
4. 評估後續 scale 的 instance type -> 若大於一開始設定的 type，則有可能造成資源浪費。但若小於一開始設定的 type，則可能造成 pods 在 schedule 的時候因為資源不夠造成無法運行
5. 盡量用 managed node group，不要用 self-managed
6. 要避免在 scale-down 某些 node 時候，有些重要的 pods 被中斷造成工作中斷且重新來過，例如執行機器學習，大數據分析的工作 pods 之類的。
    - 要避免這類事情發生，請確保在這些重要的 pods 上增加 label `cluster-autoscaler.kubernetes.io/safe-to-evict=false` 讓 CA 可以辨識的到。當這些 node 上有運行這個標籤的 pod 則該 node 就不會被 scale-down





# 關於 External DNS 工具
整合進 AWS EKS 之後，可以透過創建 ingress 設定規則，將會把配置也對應到遠端的 DNS Provider，基本上有這個配置可以不用自己再去 CloudFlare 建 Record。

參考 <https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/cloudflare.md>

參考 <https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/nginx-ingress.md>


# 參考資料
[曾經參考此影片說明其 node 種類差異](https://www.youtube.com/watch?v=yde16HpQIRo&ab_channel=BoltOps)

[關於 Pods 聯網說明 - 在 VPC CNI 的閱讀中有提到 pods 限制的問題](https://docs.aws.amazon.com/zh_tw/eks/latest/userguide/pod-networking.html)